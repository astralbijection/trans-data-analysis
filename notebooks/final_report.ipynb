{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d8b555",
   "metadata": {},
   "source": [
    "# Title\n",
    "\n",
    "Introduction section\n",
    "\n",
    "Mention algorithms, technologies, etc. We will use LDA, w2v, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f92e9e",
   "metadata": {},
   "source": [
    "## Data engineering\n",
    "\n",
    "How did we collect this data, command used, how it was combined together\n",
    "\n",
    "### Scraping\n",
    "\n",
    "This data was collected using [Twarc](https://github.com/DocNow/twarc), a command-line tool for scraping data from Twitter. We used the following bash command:\n",
    "\n",
    "```sh\n",
    "twarc --recursive search '\"transgender\" OR \"trans person\" OR \"trans people\" OR \"transmasc\" OR \"transfem\" OR \"trans man\" OR \"trans woman\" OR \"trans boy\" OR \"trans girl\" OR \"trans men\" OR \"trans women\" OR \"enby\" OR \"non binary\"' | tee /dev/tty | gzip --stdout > $OUTFILE\n",
    "```\n",
    "\n",
    "This command does the following things:\n",
    "1. Searches for every tweet from the past week that contains the specified terms.\n",
    "2. Prints it to console for easier reading.\n",
    "3. Gzips the data to reduce disk space\n",
    "4. Saves the data to wherever `$OUTFILE` is.\n",
    "\n",
    "### Combining the data\n",
    "\n",
    "Now, we will combine all the data into a single dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1f1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "\n",
    "def autopickle(path):\n",
    "    \"\"\"\n",
    "    This is a decorator to aid in pickling important things.\n",
    "    \n",
    "    If a file exists at the path, then this will load that gzipped pickle object.\n",
    "    Otherwise, it will run the function, pickle and gzip the result, and return the function.\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        if os.path.exists(path):\n",
    "            with gzip.open(path, 'rb') as file:\n",
    "                model = pickle.load(file)\n",
    "        else:\n",
    "            model = func()\n",
    "            with gzip.open(path, 'wb') as file:\n",
    "                pickle.dump(model, file)\n",
    "        return model\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def parse_twitter_datetime(dt: str):\n",
    "    return datetime.strptime(dt, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "\n",
    "def read_jsonl_gz(path):\n",
    "    with jsonlines.Reader(gzip.open(path)) as reader:\n",
    "        raw_politician_tweets = list(reader)\n",
    "\n",
    "    tweet_df = pd.DataFrame(data={\n",
    "        'tweet': [t['full_text'] for t in raw_politician_tweets],\n",
    "        'author': [t['user']['screen_name'] for t in raw_politician_tweets],\n",
    "        'date': [parse_twitter_datetime(t['created_at']) for t in raw_politician_tweets],\n",
    "        'id': [t['id'] for t in raw_politician_tweets]\n",
    "    })\n",
    "    tweet_df.set_index('id')\n",
    "\n",
    "    return tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4dc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@autopickle('../data/joined_tweets.pickle.gz')\n",
    "def tweet_df():\n",
    "    INPUTS = [\n",
    "        '../data/transgender/2021-05-06_2021-05-13.jsonl.gz',\n",
    "        '../data/transgender/2021-05-12_2021-05-20.jsonl.gz',\n",
    "    ]\n",
    "    tweet_df = pd.concat([read_jsonl_gz(path) for path in PATHS])\n",
    "    tweet_df.drop_duplicates('id', inplace=True)\n",
    "    tweet_df.to_pickle('../data/agg_trans_tweets.pickle.gz', compression='gzip')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff8815",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "TFIDF, cleaning, tokenization, pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a07041",
   "metadata": {},
   "source": [
    "## Analysis, Model Training\n",
    "\n",
    "N-grams\n",
    "\n",
    "LDA training + results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28ad2a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "reflection\n",
    "\n",
    "next steps\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
